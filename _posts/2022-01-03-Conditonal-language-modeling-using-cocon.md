---
layout: post
comments: true
title: "<i>Paper Dissection</i> : Deep dive into CoCon text generation"
header-title : "Deep dive into CoCon text generation"
date: 2022-01-03 12:00:00
tags: Deep-learning nlp short-read
---

> In this post I will be discussing about a paper on controlled text generation which was recently accepted at ICLR 2021 main conference track. This post will be removed shortly after the internal review at MLCollective as this will be submitted to ICLR 2022 blog post as conference track.
<!--more-->
{:class="table-of-content"}
* TOC
{:toc}

## Motivation 
Transformer based language models [1] Vaswani et.al, 2017 have stirred the transfer based learning in NLP and has helped greatly improve the performance of several NLP tasks. Pretraining a Language model on the huge amount of text on the web forms the basic step for the same. However, research on steering a Language model to output fine grained control over the provided content/sentiment is still being explored and has a great potential to improve quality of search engines. This paper [open-review-link](https://openreview.net/forum?id=VD_ozqvBy4W) proposes a content conditioner, which when trained auto-regressively alongside a Large pretrained language model (like GPT-2) provides the capability to control text at a fine grained level.

## Related concepts
Some of the concepts which are required for understanding the paper involve  language models, self-attention, expected value, GAN loss.

## Method of solving
### Problem representation 
In text generation, given a prompt text $$x_{:t-1}$$ of length $$t-1$$, where $$x_i$$ represents the token at $$i^{th}$$ position and $$x_{:t-1}$$ = {$$x_1 ... , x_{t-1}$$}, the following text $$x_t,...x_l$$ of length $$l-t+1$$ which can be  generated auto-regressively, can be represented as : 

$$
\begin{align}
p(x_t ... , x_l | x_1,...,x_{t-1}) &= \prod_{i = t}^{l} p(x_i | x_l,..., x_{i-1})
\end{align}
$$

Now, for controlled text generation conditioned on an attributed/ a content can be represented by the conditional probablity as : 

$$
\begin{align}
p(x_t ... , x_l | x_1,...,x_{t-1}) &= \prod_{i = t}^{l} p(x_i | \mathbf{c}, x_l,..., x_{i-1})
\end{align}
$$

where, **$$c$$** can be an attribute or a text sequence (content text) or a list of text sequences (list of content text). 

### Model Architecture
The overall architecture along with CoCon block can be represented as : 

![CoCon block*]({{ '/assets/images/CoCon-1.png' | relative_url }})
{: style="width: 100%;" class="center"}
*Figure 1. CoCon block sandwiched between $$LM_{\alpha}$$* and $$LM_{\beta}$$ 
{:.image-caption} 

here, 

1. <strong>$$LM_{\alpha}$$</strong> - The transformer block before CoCon block - This acts as a feature extractor in the input embeddings and outputs intermediate breakpoint. 

$$
    h_{t-1} = LM_{\alpha}(x_{:t-1})
$$

2. <strong>$$LM_{\beta}$$</strong>  -  This block takes in interemediate representation at breakpoint and outputs the logits $$o_t$$, which then can be used for generating the next token $$x_t$$.  

$$
o_{t} = LM_{\beta}(h_{:t-1})
$$


These $$LM_{\alpha}$$ and $$LM_{\beta}$$ if joined comprises a GPT-2 Model [2] Radford et.al, 2019. Clearly we can see that, by utlizing this midpoint representation, we can control next token logit - <strong>$$o$$ </strong>. Per Figure 1, we utilize the CoCon block to combine the intermediate representation  $$h_{t-1}$$ and the representation of the content - $$h_{:l_c}$$ from $$LM_{\alpha}$$ (where, $$l_c$$ is the length of the content **c**) on which the prompt is to be conditoned on to generate newly created intermediate embeddings $$h^{'}_{t-1}$$. 

$$
h^{'}_{t-1} = CoCon(h_{:l_c}, h_{t-1})
$$

The CoCon layer with the two inputs as mentioned above is a single transformer block which creates Query, Key and Value vectors for the two inputs. The Query, Key and Value vector for $$h_{t-1}$$ is generated by linear transformation with the learnable weight matrices and has dimension  $$(t-1) \times d$$ where $$(t-1)$$ is the sequence length and $$d$$ is the embedding dimension.

$$
Q, K, V \in \mathbb{R}^{(t-1) \times d}
$$

To make the prompt text $$x_{t-1}$$ to get influenced by the content $$c$$, the key and value vectors ($$K^c$$ and $$V^c$$ ) of the content's intermediate representation $$h^{c}_{:l_c}$$ is also generated : 

$$
    K^c, V^c \in \mathbb{R}^{l_c \times d}
$$

These content representation's key and value vectors are combined with the $$K, V$$ vectors of the prompt text so that the Query vector $$Q$$ of the prompt text $$p$$ gets influenced by the Key vector of the subword tokens (the prompt text as well as conditioned text) 

$$
    K^{'}= [K^c, K] \in \mathbb{R}^{(l_c + t - 1) \times d} \tag{1}
    
$$

$$
    V^{'} = [V^c, V] \in \mathbb{R}^{(l_c + t - 1) \times d} \tag{2}
$$

By utilizing these vectors, the CoCon block creates the attention weights $$W = QK^{'T} \in \mathbb{R}^{(t - 1) \times (l_{c} + (t - 1))}$$. The attention weights then will be mutiplied with the value vectors to create the embeddings. These embeddings will be fed to a feed-forward layer to get the final output, $$h^{'}_{t-1}$$. 

$$
    A = Softmax(QK^{'T})V^{'} \in \mathbb{R}^{(t - 1) \times d }
$$

$$
    h^{'}_{t-1} = FF(A) \in \mathbb{R}^{(t - 1) \times d }
$$

if there are $$n$$ content inputs, the eq. 1 and 2 can be changed to : 

$$
    K^{'} = [K^{c^1} K^{c^2} ... K^{c^n}; K] \; \;   \; \; V^{'} = [V^{c^1} V^{c^2} ... V^{c^n}; V]  
$$

and the flexibilty of the CoCon enables the rest of the equation to be the same.

The final representation, $$h^{'}_{t-1}$$ will be combined with the representation prior to $$(t-1)$$ i.e $$h_{:t-2}$$ in $$LM_{\beta}$$  which outputs a token logit $$\widetilde{o}_t $$ and so on to have a conditoned output sequence.

$$
    \widetilde{o}_t = LM_{\beta}([h_{t-2}, h^{'}_{t-1}]) \tag{3}
$$

The softmax on $$\widetilde{o}_t$$ provides the token $$\widetilde{x}_t$$ different from $$x_t$$ as it has been conditoned on content input $$c$$.

$$
    p_{\theta, \psi}(\widetilde{x}_t|c, x_{t-1}) = Softmax(\widetilde{o}_t)
$$

here $$\theta$$ represents the CoCon block parameters and $$\psi$$ represents the LM parameters.

### Model training
CoCon block is trained self-supervisedly with the output generated by the Language model which is used in adjacent to the CoCon block. Given any text sequence of length $$l$$ $$x = [x_1, x_2, .... x_{t-1}, x_{t}, .... , x_l]$$, the sequence can be divided into two parts where, 

$$
    x^a = {x_1, ... x_{t-1}}
$$

$$
    x^b = {x_{t}, ... , x_l}
$$

Where, $$x = [x^a; x^b]$$. In real world, there are multiple sentences that can follow $$x^a$$. So without the information about $$x_b$$ the probablity to re-construct $$x^b$$ from $$x^a$$ is very low. To alleviate and incorporate conditional modeling, the paper introduces four losses : 

**Self reconstruction loss:**
For reconstrucing original sentence $$x$$, the cocon block is provided with intermediate representation of both $$x$$ and $$c = x_b$$.

$$
    \mathbf{h_{:l}} = LM_{\alpha}(x_{:l}), \; \; \; \mathbf{h^{(c)}_{:l_c}} = LM_{\alpha}(x_{t:l})
$$

The CoCon block then, by utilizing the representation $$\mathbf{h^{(c)}_{:l_c}}$$ generates the intermediate representation auto-regressively $$\forall i \geq t-1$$. (Here all the represenation after $$i$$ will be masked out so that CoCon does not see the future terms in $$h_{:l}$$)


$$
    h^{'}_{i} = CoCon(h^{(c)}_{:l_c},  h_{:i}), \; \; \; \forall i \geq t-1
$$

Simlar to (3), they generate the next token logit from $$LM_{\beta}$$ after applying the Softmax operation.

$$
    \widetilde{o}_{i+1} = LM_{\beta}([h_{:t-2}, \; h^{'}_{t:i}]),  \; \; \; p_{\theta, \psi}(\widetilde{x}_{i+1}|c, x_{:i}) = Softmax(\widetilde{o}_{i+1}), \; \; \; \forall i \geq t-1
$$

Now the training loss for self restruction is the sum of log likelihood loss $$\forall i \in (t, \ l) $$ where, the conditioned text is the second part of text sequence $$x$$ i.e $$x_b$$ and mathematically represented as: 

$$
    {L_{self}} = - \sum_{i = t}^{l}log \ p_{\psi, \theta}(x_i | (c = x_b), \{x_1, ..., x_{i-1}\})
$$

**Null content loss**
The main aim of this loss is to make the text generation as fluent as possible. This loss removes the hard dependency of the presence of content in generating the text from the prompt text (In any absence of content). 

$$
    {L_{null}} = - \sum_{i = t}^{l}log \ p_{\psi, \theta}(x_i | (c = \emptyset), \{x_1, ..., x_{i-1}\})
$$

**Cycle reconstruction loss**
We can express the CoCon's autoregressive generation as : 

$$
    y = f_{\theta, \psi}(c, p)
$$

where, $$c$$ is the content, and $$p$$ is the prompt text. To make the CoCon block more generalizable for text where both $$c$$ and $$p$$ are from
divergent sources, the paper uses two sentences $$x$$ and $$x^{'}$$ to creates two pairs of $$c$$ and $$p$$ respectively. Splitting both the text sequence x and x' : 

$$
    x = [x^{a}; x^{b}]
$$

$$
    x^{'} = [x^{'a}; x^{'b}]
$$

Steps : 
* Generate text sequence $$y_{x, x^{'}}$$ with content input $$c$$ from $$x$$ and prompt text from $$x^{'}$$.

$$
    y_{x, x^{'}} = f_{\theta, \psi}((c = x^b), p = x^{'a})
$$

* Next step involves using $$y_{x, x^{'}}$$ as a content and $$x^{a}$$ as the prompt text which generates $$y_{cycle}$$.

$$
    y_{cycle} = f_{\theta, \psi}((c = y_{x, x^{'}}), p = x^{a})
$$

now, $$x_b$$ acts as a training label for the generated $$y_{cycle}$$ and provides us the cycle reconstruciton loss for training the CoCon block. 

$$

L_{cycle} = - \sum_{i = t}^{l}log \ p_{\psi, \theta}(y_{cycle} = x^{b} | (c = y_{x, x^{'}}), (p = x^a))

$$

**Adversarial loss**

To match the output representation $$y$$ with those of training samples $$x$$, The generator (here the LM with CoCon) is made to train adversarially with a $$f_{disc}$$ network. The experission used follows by the expression from [3] Goodfellow et.al, 2014 

$$
    L_{adv} = \mathbb{E}_{x}[log \ f_{disc}(LM_{\alpha}(x))] + \mathbb{E}_{y}[log \ (1 - f_{disc}(LM_{\alpha}(y)))] \tag{4}
$$

The $$f_{disc}$$ is initially made to train to maximize the loss so that it can distinguish the two represenation better (Real representation : $$x$$ and generated representation $$y$$). This makes the Generator to output represenation similar to training samples. The $$f_{disc}$$ is parameterized by $$\phi$$ and its training objective is to maximize the the $$L_{adv}$$
 
$$
    \phi^{*} = \underset{\phi}{arg \ max} \ L_{adv}
$$

This part of the training acts as a method of strengthening the discriminator to distinguish between input and generated representations. 

**Full training**

Full training of CoCon block is done by minimizing all the four loss terms through stochastic gradient descent. 

$$
    \theta^{*} = \underset{\theta}{arg \ max} (\lambda_{self}L_{self} + \lambda_{null}L_{null} + \lambda_{cycle}L_{cycle} + \lambda_{adv}L_{adv})
$$

here, the constant $$\lambda$$ can be used to weigh the losses. The loss $$L_{adv}$$ acts as a part of adverserial training where it pushes the CoCon block to generate similar intermediate representation of training input and the generated $$y$$ by minimzing the loss as per eq4.

## Results 
Will be written with the hand crafted examples for foolproofing.

## References

[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017.

[2] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019 

[3] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672–2680, 2014.

